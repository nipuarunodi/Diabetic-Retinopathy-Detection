{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5pOnEOrTkh1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import cv2\n",
        "from pandas.errors import EmptyDataError\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, BatchNormalization, Input \n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.models import Model\n",
        "\n",
        "drive.mount('/drive')\n",
        "\n",
        "dataset = pd.read_csv('/drive/My Drive/Diabatic_Retinopathy_Data/train.csv',error_bad_lines=False)\n",
        "image_folder_path = \"/drive/My Drive/Diabatic_Retinopathy_Preprocessed_Images\"\n",
        "\n",
        "one_hot_labels = pd.Series(dataset['diagnosis'])\n",
        "one_hot_labels = pd.get_dummies(one_hot_labels, sparse = True)\n",
        "one_hot_labels = np.asarray(one_hot_labels)\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for index, row in dataset.iterrows():\n",
        "  image_path = os.path.join(image_folder_path, row['id_code']+'.png')\n",
        "  image = cv2.imread(image_path)\n",
        "  if image is None:\n",
        "    print(\"No Image : \", image_path)\n",
        "  else: \n",
        "    x.append(image)\n",
        "    y.append(one_hot_labels[index])\n",
        "    print(index)\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, train_size = 0.8, random_state=42)\n",
        "\n",
        "visible = Input(shape=(300,300,3))\n",
        "\n",
        "conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "conv2 = Conv2D(64, kernel_size=4, activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "conv3 = Conv2D(128, kernel_size=4, activation='relu')(pool2)\n",
        "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "conv4 = Conv2D(64, kernel_size=4, activation='relu')(pool3)\n",
        "pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "conv5 = Conv2D(32, kernel_size=4, activation='relu')(pool4)\n",
        "pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
        "\n",
        "flat = Flatten()(pool5)\n",
        "\n",
        "hidden1 = Dense(128, activation='relu')(flat)\n",
        "hidden2 = Dense(128, activation='relu')(hidden1)\n",
        "hidden3 = Dense(64, activation='relu')(hidden2)\n",
        "output = Dense(5, activation='softmax')(hidden3)\n",
        "model = Model(inputs=visible, outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']) \n",
        "\n",
        "y_train_raw = np.asarray(y_train).astype(np.float32)\n",
        "x_train_raw = np.asarray(x_train).astype(np.float32)\n",
        "x_valid_raw = np.asarray(x_valid).astype(np.float32)\n",
        "y_valid_raw = np.asarray(y_valid).astype(np.float32)\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_raw, \n",
        "    y_train_raw,\n",
        "    validation_data = (x_valid_raw, y_valid_raw),\n",
        "    batch_size = 32,\n",
        "    epochs=50,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "score = model.evaluate(x_valid_raw, y_valid_raw, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "gikWmxYaUsL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Graphs\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Retrieve a list of list results on training and validation data\n",
        "# sets for each training epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\n",
        "plt.plot(epochs, acc)\n",
        "plt.plot(epochs, val_acc)\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss)\n",
        "plt.plot(epochs, val_loss)\n",
        "plt.title('Training and validation loss')"
      ],
      "metadata": {
        "id": "gJR1Cm96UyXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the Model\n",
        "model.save('/drive/My Drive/trained_models/approach_2')"
      ],
      "metadata": {
        "id": "d5NW_fX8U7s0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}